{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059172ec-bcdb-4d69-9926-8eb064c6aa2a",
   "metadata": {},
   "source": [
    "### Chat with your Logs with Llama3 and Ollama\n",
    "\n",
    "Adapted from original Code by Sascha Retter (https://blog.retter.jetzt/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90f6911e-5d51-4438-a76c-6be0093d56fd",
   "metadata": {},
   "source": [
    "##### Chat with local Llama3 Model via Ollama in KNIME Analytics Platform — Also extract Logs into structured JSON Files\n",
    "https://medium.com/p/aca61e4a690a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c8c70-cc08-43c0-8867-9d624f0219a4",
   "metadata": {},
   "source": [
    "##### Ask Questions from your CSV with an Open Source LLM, LangChain & a Vector DB\n",
    "https://www.tetranyde.com/blog/langchain-vectordb\n",
    "\n",
    "##### Document Loaders in LangChain\n",
    "https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n",
    "\n",
    "##### Unleashing Conversational Power: A Guide to Building Dynamic Chat Applications with LangChain, Qdrant, and Ollama (or OpenAI’s GPT-3.5 Turbo)\n",
    "https://medium.com/@ingridwickstevens/langchain-chat-with-your-data-qdrant-ollama-openai-913020ec504b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d60e71-b2fc-4a0b-b274-b7cb9ff452ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Document Loaders in LangChain\n",
    "# https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "model = \"llama3:instruct\" # model needs already be available, already pulled with for example 'ollama run llama3:instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dc8613f-4331-4b0a-b7b2-dfb6b63df79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"What would be the best set of JSON columns to extract data from these Logfiles in a systematic way? Can you write a prompt?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd051ab-7018-4dd8-b888-099fd42f6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can omit the cource_column\n",
    "# https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n",
    "loader = CSVLoader(\"../data/sample_logs.csv\", source_column=\"Prompt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d5605e0-afa3-4e88-8fc9-677f9073b1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee6a585f-495e-417f-92aa-430aa6e5a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf94ddb8-95c1-4dfb-9a5b-c58103f6a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma.from_documents(\n",
    "        documents,  embedding=oembed, persist_directory=\"../data/vectorstore/chroma_logfiles\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f6d86a0-f5b2-446f-8958-2b1242ddd51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.chroma.Chroma"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chroma_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "675c6cc1-cc60-4ffc-b30c-6f4450f2db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "chroma_db = Chroma(persist_directory=\"../data/vectorstore/chroma_logfiles\", embedding_function=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81350b00-200c-419c-9838-5ea44914e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.chroma.Chroma"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chroma_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b039ebb-76bc-4598-9cd9-4bdc29a0f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM model llama3:instruct\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "llm = Ollama(model=model,\n",
    "            verbose=True,\n",
    "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "print(f\"Loaded LLM model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9042a07c-6234-47ea-a7ff-25b78e1f01d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided log files, I can suggest the following set of JSON columns that could be extracted in a systematic way:\n",
      "\n",
      "1. `timestamp`: Extract the timestamp (e.g., \"03/22 08:53:38\") from each log entry.\n",
      "2. `log_level`: Determine the log level (e.g., \"TRACE\") and extract it as a separate column.\n",
      "3. `component`: Identify the component or module that generated the log message (e.g., \"router_forward_getOI\", \"rsvp_flow_stateMachine\", etc.) and extract it as a separate column.\n",
      "4. `source_address`: Extract the source IP address (e.g., \"9.67.116.98\") from each log entry.\n",
      "5. `out_inf`: Extract any relevant information about the output interface or network interface (e.g., \"9.67.116.98\").\n",
      "6. `gateway`: Extract the gateway IP address (e.g., \"0.0.0.0\") if present in the log entries.\n",
      "7. `event_type`: Identify the type of event that triggered the log message (e.g., \"reentering state RESVED\", \"received event from RAW-IP on interface\").\n",
      "\n",
      "Here's a sample JSON schema based on these extracted columns:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"timestamp\": {\n",
      "    \"type\": \"string\"\n",
      "  },\n",
      "  \"log_level\": {\n",
      "    \"type\": \"string\"\n",
      "  },\n",
      "  \"component\": {\n",
      "    \"type\": \"string\"\n",
      "  },\n",
      "  \"source_address\": {\n",
      "    \"type\": \"string\"\n",
      "  },\n",
      "  \"out_inf\": {\n",
      "    \"type\": \"string\"\n",
      "  },\n",
      "  \"gateway\": {\n",
      "    \"type\": [\"null\", \"string\"]\n",
      "  },\n",
      "  \"event_type\": {\n",
      "    \"type\": \"string\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "Please note that the actual JSON schema might need to be adjusted based on specific requirements and data types."
     ]
    }
   ],
   "source": [
    "# Initialize the RetrievalQA chain with the vector store retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=chroma_db.as_retriever(),\n",
    ")\n",
    "\n",
    "# Use the 'invoke' method to handle the query instead of '__call__'\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97a1ba-5f4c-4bd4-b2a3-d20c00f07c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ddf19-9c62-47d5-b766-d3dab137c1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
