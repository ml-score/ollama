{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059172ec-bcdb-4d69-9926-8eb064c6aa2a",
   "metadata": {},
   "source": [
    "### Chat with your unstructured Text Files with Llama3 and Ollama\n",
    "\n",
    "Some code inspired by Sascha Retter (https://blog.retter.jetzt/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90f6911e-5d51-4438-a76c-6be0093d56fd",
   "metadata": {},
   "source": [
    "##### Chat with local Llama3 Model via Ollama in KNIME Analytics Platform — Also extract Logs into structured JSON Files\n",
    "https://medium.com/p/aca61e4a690a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c8c70-cc08-43c0-8867-9d624f0219a4",
   "metadata": {},
   "source": [
    "##### Ask Questions from your CSV with an Open Source LLM, LangChain & a Vector DB\n",
    "https://www.tetranyde.com/blog/langchain-vectordb\n",
    "\n",
    "##### Document Loaders in LangChain\n",
    "https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n",
    "\n",
    "##### Unleashing Conversational Power: A Guide to Building Dynamic Chat Applications with LangChain, Qdrant, and Ollama (or OpenAI’s GPT-3.5 Turbo)\n",
    "https://medium.com/@ingridwickstevens/langchain-chat-with-your-data-qdrant-ollama-openai-913020ec504b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0d60e71-b2fc-4a0b-b274-b7cb9ff452ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Document Loaders in LangChain\n",
    "# https://medium.com/@varsha.rainer/document-loaders-in-langchain-7c2db9851123\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import utils as chromautils\n",
    "\n",
    "# from langchain.llms import Ollama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # the standard embedding model for\n",
    "model = \"llama3:instruct\" # model needs already be available, already pulled with for example 'ollama run llama3:instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8a38559-33a4-496a-a338-778244ae59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy configuration\n",
    "proxy = \"http://proxy.my-company.com:8080\"  # Replace with your proxy server and port\n",
    "proxy = \"\"\n",
    "os.environ['http_proxy'] = proxy\n",
    "os.environ['https_proxy'] = proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dc8613f-4331-4b0a-b7b2-dfb6b63df79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = f\"What does Hamlet say to his mother? Can you give the source?\"\n",
    "\n",
    "question = f\"What is the first person Hamlet does kill? Can you give the source?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76b3b972-1cea-4b10-906f-e09214a187de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing your log files. Note: if they have .CSV endings other document loaders might be better\n",
    "text_files_directory = \"../documents/shakespeare/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a620bdf-6b7a-485b-a3b9-cfae13c01567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"../documents/shakespeare/A Midsummer Night's Dream.txt\", '../documents/shakespeare/Hamlet, Prince of Denmark.txt', '../documents/shakespeare/King Lear.txt', '../documents/shakespeare/Macbeth.txt', '../documents/shakespeare/Sonnets by William Shakespeare.txt']\n"
     ]
    }
   ],
   "source": [
    "def list_text_files(directory):\n",
    "    \"\"\"List all TXT files in the given directory.\"\"\"\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    # Filter out all files that end with '.txt'\n",
    "    # and file.startswith('coffee')\n",
    "    \n",
    "    # txt_files = [file for file in files if file.endswith('.txt')]\n",
    "    txt_files = [os.path.join(directory, file) for file in files if file.endswith('.txt')]\n",
    "    return txt_files\n",
    "\n",
    "# Specify the directory to search for PDF files\n",
    "txt_files = list_text_files(text_files_directory)\n",
    "print(txt_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b411b6d-624c-4f47-af1e-8cea04517459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a9699459\\AppData\\Local\\miniforge3\\envs\\py3_knime_llama\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/langchain-ai/langchain/issues/8556#issuecomment-1806835287\n",
    "\n",
    "# Load and embed the content of the log files\n",
    "def load_and_embed_files(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        loader = UnstructuredFileLoader(file_path, mode=\"elements\")\n",
    "        documents.extend(loader.load())\n",
    "        documents = chromautils.filter_complex_metadata(documents)\n",
    "    return documents\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "# embedding_model = SentenceTransformerEmbeddings(model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1964e823-d65f-4249-a679-be9616fcdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and embed the log files\n",
    "documents = load_and_embed_files(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9908d7a7-3bba-44e3-b5bc-b9723a21e0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf077c85-eaf3-42fe-aa11-b762fbb9314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to store the Chroma vector store (in SQLite format)\n",
    "v_path_vector_store = '../data/vectorstore/shakespeare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b028eb79-0b6d-48ea-b941-e885495b6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector store from the documents / logs you provided\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents, \n",
    "    embedding=embedding_model, \n",
    "    persist_directory=v_path_vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11873841-2f99-4653-8e18-f7cb8cf07fc2",
   "metadata": {},
   "source": [
    "#### Use the stored Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "675c6cc1-cc60-4ffc-b30c-6f4450f2db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vectorstore from disk\n",
    "chroma_db = Chroma(persist_directory=v_path_vector_store, embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81350b00-200c-419c-9838-5ea44914e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_community.vectorstores.chroma.Chroma"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chroma_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b039ebb-76bc-4598-9cd9-4bdc29a0f320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM model llama3:instruct\n"
     ]
    }
   ],
   "source": [
    "# define the LLM - if you just want the result and not see it being printed out set verbose=False\n",
    "llm = Ollama(model=model,\n",
    "            verbose=True,\n",
    "            callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "\n",
    "print(f\"Loaded LLM model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b173f48-b692-4c39-b8f0-75f0b6961af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm happy to help!\n",
      "\n",
      "Based on the context provided, it seems that the question is about the character Hamlet from Shakespeare's play \"Hamlet\". If I understand correctly, the question asks what the first person Hamlet kills.\n",
      "\n",
      "From my knowledge of the play, I can tell you that Hamlet's first kill is Polonius. This occurs in Act 3, Scene 3, when Hamlet mistakes Polonius for a snake and stabs him through the arras (curtain).\n",
      "\n",
      "Source: Shakespeare, W. (1603). Hamlet.\n",
      "\n",
      "Please let me know if I'm correct or if you'd like me to clarify anything!"
     ]
    }
   ],
   "source": [
    "# Initialize the RetrievalQA chain with the vector store retriever\n",
    "retriever = chroma_db.as_retriever(search_kwargs={\"k\": 2})  # Use the number of documents to retrieve\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# Use the 'invoke' method to handle the query\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475d291-6ef8-44b7-93ee-8905337d8c3e",
   "metadata": {},
   "source": [
    "#### Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2053f1-ef5a-4f1c-bf7e-d1b22217cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_model = Ollama(model=model, verbose=False)  # Disable verbose for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ddf19-9c62-47d5-b766-d3dab137c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the instruction and log file prompts\n",
    "v_instruct = \"\"\"Instructions:\n",
    "\"\"\"\n",
    "\n",
    "v_prompt = \"\"\"Question:\n",
    "\"\"\"\n",
    "\n",
    "# Combine the instruction and prompt\n",
    "combined_prompt = v_instruct + \"\\n\" + v_prompt\n",
    "\n",
    "# Print the instruction and log file prompt\n",
    "# print(v_instruct)\n",
    "# print(v_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97a1ba-5f4c-4bd4-b2a3-d20c00f07c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LLM to process the combined prompt\n",
    "# response = llm_model(combined_prompt)\n",
    "response = llm(combined_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94e95a-066e-4b43-a384-502eb75fe2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c0e9e-8139-4718-912a-c2bbe0cfb460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
